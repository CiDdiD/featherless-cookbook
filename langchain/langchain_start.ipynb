{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating with LangChain\n",
    "LangChain, the most widely adopted of these libraries, provides developers with powerful tools and patterns for managing complex prompts and conversational state. This notebook shows how to make your first API call integrating Featherless with the LangChain library.\n",
    "\n",
    "## Prerequisites\n",
    "1. Sign up for an account at [Featherless](https://featherless.ai/register)\n",
    "2. Subscribe to a plan and get your API key from [API Keys](https://featherless.ai/account/api-keys)\n",
    "## Setup\n",
    "First, let's import the required libraries and set up our API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "# Set your API key\n",
    "FEATHERLESS_API_KEY= \"your-api-key-here\" # Replace with actual API key\n",
    "# Alternatively, you can set it as an environment variable\n",
    "# FEATHERLESS_API_KEY = os.getenv(\"FEATHERLESS_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a LangChain Chat Model\n",
    "Now we'll create a ChatOpenAI instance configured to use Featherless's API. We'll set up the model to use Meta's Llama 3 8B Instruct model through Featherless's API endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    api_key='{FEATHERLESS_API_KEY}',\n",
    "    base_url=\"https://api.featherless.ai/v1\",\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a Conversation\n",
    "Let's create a simple conversation that asks the model to translate English to French. We'll structure this using a system message to set the behavior and a human message containing the text to translate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"I love programming.\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the response\n",
    "Now we can send our messages to the model and get the translation. The `invoke()` method handles the API call and returns the model's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_msg = llm.invoke(messages)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
